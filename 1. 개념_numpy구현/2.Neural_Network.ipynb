{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    " 공부와 수면시간과 성적의 관계를 통해 가장 기본적 딥러닝 모델을 numpy를 통해 구축해보자\n",
    "   \n",
    " <b>참고</b>  \n",
    " https://www.youtube.com/watch?v=bxe2T-V8XRs&list=RDCMUConVfxXodg78Tzh5nNu85Ew&index=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1 - 데이터 정의\n",
    "   \n",
    "x - 공부시간, 수면시간  \n",
    "y - 성적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[3, 5],[5, 1],[10, 2]])\n",
    "y = np.array([[0.75], [0.82], [0.93]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2 - forward propagation\n",
    " forward propagation 은 아래 (1), (2), (3), (4) 수식을 걸쳐서 이루어진다.  \n",
    " 모델구조는 hidden layer - 1, neural - 3로 구성되며 f는 활성함수(activation function)으로 sigmoid를 사용할 것이다.\n",
    "\n",
    " $$z_2 = XW_1 \\ (1)$$\n",
    " $$a_2 = f(z_2) \\ (2)$$\n",
    " $$z_3 = a_2W_2 \\ (3)$$\n",
    " $$ \\hat y = f(z_3) \\ (4)$$\n",
    " $$loss = L(y,\\hat y) \\ (5)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self, input_layer_size, output_layer_size, hidden_layer_size):\n",
    "        self.input_layer_size = input_layer_size\n",
    "        self.output_layer_size = output_layer_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.W1 = np.random.randn(self.input_layer_size, self.hidden_layer_size)\n",
    "        self.W2 = np.random.randn(self.hidden_layer_size, self.output_layer_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        z2 = X.dot(self.W1)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        z3 = a2.dot(self.W2)\n",
    "        y_hat = self.sigmoid(z3)\n",
    "        return y_hat\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    \n",
    "def MSE(y, y_hat):\n",
    "    return np.power(y-y_hat,2)/2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01553813],\n",
       "       [0.00461452],\n",
       "       [0.0185075 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = Neural_Network(2,1,3)\n",
    "loss = MSE(y, nn.forward(x))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step3 - back propagation  \n",
    " \n",
    " 각 weghit 변수 마다 gradient를 적용하여 loss가 가장 적은 weghit 값을 찾아야한다.  \n",
    " gradinet를 사용하는 이유는 모든 weghit를 전체 탐사하여 최적점을 찾기에는 비용(시간)이 기하급수적으로 늘어난다.    \n",
    " gredient를 통해 각 loss를 최소화 시키는 w 방향(기울기)를 찾아 조절함을 통해 최소한의 비용으로 모델 최적화를 이뤄낸다.  \n",
    " 결국 loss를 구하고 해당 loss를 최소화 시키는 w2를 구하고 w2를 최적화 시키는 w1을 구하는 식으로 output layer로 부터 input layer까지 역으로 w 값 최적화를 진행한다.   \n",
    " 이러한 방법을 back propagation이라고 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {\\partial \\ loss \\over \\partial \\ W1} = \\left[\n",
    "\\begin{matrix}\n",
    "    {\\partial \\ loss \\over \\partial \\ w_{11}} & {\\partial \\ loss \\over \\partial \\ w_{12}} & {\\partial \\ loss \\over \\partial \\ w_{13}} \\\\\n",
    "    {\\partial \\ loss \\over \\partial \\ w_{21}} & {\\partial \\ loss \\over \\partial \\ w_{22}} & {\\partial \\ loss \\over \\partial \\ w_{23}} \\\\\n",
    "\\end{matrix}\n",
    "\\right] $$\n",
    "\n",
    "$$ {\\partial \\ loss \\over \\partial \\ W2} = \\left[\n",
    "\\begin{matrix}\n",
    "    {\\partial \\ loss \\over \\partial \\ w_{11}}  \\\\\n",
    "    {\\partial \\ loss \\over \\partial \\ w_{21}}  \\\\\n",
    "    {\\partial \\ loss \\over \\partial \\ w_{31}}  \\\\\n",
    "\\end{matrix}\n",
    "\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step3_1 - sum rule in differentiation\n",
    " loss 를 구하기 위한 MSE 수식을 가져온다.   \n",
    " 미분의 합은 합의 미분과 같기 때문에 미분을 취하고 합을 취해도 된다.\n",
    " \n",
    " $${\\partial \\ loss \\over \\partial \\ W_2} = {\\partial \\sum {1 \\over 2} (y - \\hat y)^2 \\over \\partial W_2} $$\n",
    "   \n",
    " $${\\partial \\ loss \\over \\partial \\ W_2} = \\sum{\\partial \\ {1 \\over 2} (y - \\hat y)^2 \\over \\partial W_2} $$\n",
    " \n",
    "#### step3_2 - chain rule1\n",
    "  아래 chain rule 수식을 이용하여 loss 함수의 제곱을 제거해준다.\n",
    "    \n",
    " $$ ======================== 베이스수식 ========================  $$\n",
    "   \n",
    " $$(g \\cdot x)^\\prime = (g \\cdot x)g^\\prime$$\n",
    "   \n",
    " $$ex) {\\partial x \\over \\partial}(3x^2+7x)^2 = 2(3x^2+7x)(6x+7)$$\n",
    " \n",
    "$$ ======================== 아래 적용 ========================  $$\n",
    " \n",
    " $${\\partial \\ loss \\over \\partial \\ W_2} = 2* {1 \\over 2} (y-\\hat y)({y \\over \\partial W_2} - {\\hat y \\over \\partial W_2})$$\n",
    "   \n",
    "  $${\\partial \\ loss \\over \\partial \\ W_2} =  1 (y-\\hat y)(0 - {\\hat y \\over \\partial W_2})$$\n",
    "  \n",
    "  $${\\partial \\ loss \\over \\partial \\ W_2} =  -(y-\\hat y)({\\hat y \\over \\partial W_2})$$\n",
    " \n",
    "#### step3_3 - chain rule 2\n",
    "  \n",
    " $$ ======================== 베이스수식 ========================  $$\n",
    " \n",
    " $${\\partial y \\over \\partial x} = {\\partial y \\over \\partial z}\\cdot{\\partial z \\over \\partial x}$$\n",
    "   \n",
    "$$ ======================== 아래 적용 ========================  $$\n",
    "   \n",
    " $${\\partial \\ loss \\over \\partial \\ W_2} =  -(y-\\hat y)({\\partial \\hat y \\over \\partial z_3}\\cdot{\\partial z_3 \\over \\partial W_2})$$\n",
    "   \n",
    " $$ {\\partial \\hat y \\over \\partial z_3} = {\\partial f(z_3) \\over \\partial z_3}$$\n",
    "   \n",
    "   \n",
    "#### step3_4 - f(z_3) 미분 (sigmoid)\n",
    "\n",
    " $$ ======================== 베이스수식 ========================  $$\n",
    "  \n",
    " $$ f(x) = {u \\over v} $$ \n",
    "   \n",
    " $$ f^\\prime(x) = {u^\\prime v - uv^\\prime \\over v^2} $$ \n",
    " \n",
    " $$ ======================== 아래 적용 ========================  $$\n",
    "   \n",
    " $$ f(z) = {1 \\over 1+e^{-z}}$$ \n",
    "   \n",
    " $$ f^\\prime(z) = {0(1+e^{-z}) - 1(0-e^{-z}) \\over (1+e^{-z})^2}$$\n",
    "   \n",
    " $$ f^\\prime(z) =  {e^{-z} \\over (1+e^{-z})^2}$$\n",
    "\n",
    "  $${\\partial \\ loss \\over \\partial \\ W_2} =  -(y-\\hat y)( \\ f^\\prime(z_3)\\cdot{\\partial z_3 \\over \\partial W_2} \\ )$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return np.exp(-z)/np.power(1+np.exp(-z),2)\n",
    "\n",
    "def sigmoid(z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "sig = sigmoid(np.array([i for i in range(-10, 10)]))\n",
    "sig_prime = sigmoid_prime(np.array([i for i in range(-10, 10)]))\n",
    "\n",
    "plt.title('sigmoid differentiation')\n",
    "plt.plot(sig)\n",
    "plt.plot(sig_prime)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step3_5 - w2 미분값 추출\n",
    "\n",
    " $$ ======================== 베이스수식 ========================  $$\n",
    "   \n",
    " $$ 1차 함수의 미분  $$\n",
    "   \n",
    " $$ {\\partial \\ aw \\over \\partial \\ w}  = a^T $$\n",
    " \n",
    " $$ ======================== 아래 적용 ========================  $$\n",
    " \n",
    " $${\\partial \\ loss \\over \\partial \\ W_2} =  -(y-\\hat y)( \\ f^\\prime(z_3)\\cdot{\\partial z_3 \\over \\partial W_2} )$$\n",
    "  \n",
    " $${\\partial \\ loss \\over \\partial \\ W_2} =  -(y-\\hat y)( \\ f^\\prime(z_3)\\cdot{\\partial \\ a_2W_2 \\over \\partial W_2} )$$\n",
    "   \n",
    "  $${\\partial \\ loss \\over \\partial \\ W_2} =  -{a_2^T}\\cdot(y-\\hat y)\\ f^\\prime(z_3)$$\n",
    "  \n",
    "  \n",
    "#### step3_6 - w1 미분값 추출\n",
    "  $${\\partial \\ loss \\over \\partial \\ W_1} = -(y-\\hat y) \\ ({\\partial \\hat y \\over W_1}) = -(y-\\hat y)({\\partial \\hat y \\over \\partial z_3} \\cdot {\\partial z_3 \\over \\partial a_2} \\cdot {\\partial a_2 \\over \\partial z_2} \\cdot {\\partial  z_2 \\over \\partial W_1})$$\n",
    "  \n",
    "  $${\\partial \\ loss \\over \\partial \\ W_1} = -(y-\\hat y)({\\partial f(z_3) \\over \\partial z_3} \\cdot {\\partial (a_2 \\cdot W_2) \\over \\partial a_2} \\cdot {\\partial f(z_2) \\over \\partial z_2} \\cdot {\\partial  (X \\cdot W_1) \\over \\partial W_1})$$\n",
    "    \n",
    "  $${\\partial \\ loss \\over \\partial \\ W_1} = -(y-\\hat y)({f^\\prime(z_3)} \\cdot { W_2^T} \\cdot {f^\\prime(z_2)} \\cdot {\\partial  (X \\cdot W_1) \\over \\partial W_1})$$\n",
    "    \n",
    "  $${\\delta_1 = -(y-\\hat y)\\ f^\\prime(z_3)}$$\n",
    "    \n",
    "  $${\\delta_2 = \\delta_1 \\cdot { W_2^T} \\cdot {f^\\prime(z_2)}}$$\n",
    "  \n",
    "  $${\\partial \\ loss \\over \\partial \\ W_1} = \\delta_2{\\partial  (X \\cdot W_1) \\over \\partial W_1}= X^T \\cdot \\delta_2$$\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step code\n",
    "x = np.array([[3, 5],[5, 1],[10, 2]])\n",
    "y = np.array([[0.75], [0.82], [0.93]])\n",
    "w1 = np.random.randn(2,4)\n",
    "w2 = np.random.randn(4,1)\n",
    "\n",
    "\n",
    "z1 = x.dot(w1)\n",
    "a1 = sigmoid(z1)\n",
    "z2 = a1.dot(w2) \n",
    "y_hat = sigmoid(z2)\n",
    "loss = MSE(y,y_hat)/2\n",
    "\n",
    "\n",
    "delta1 = (y_hat-y)*sigmoid_prime(z2)\n",
    "w2_gred = a1.T.dot(delta1)\n",
    "\n",
    "delta2 = np.dot(delta1, w2.T) * sigmoid_prime(z1)\n",
    "w1_gred = np.dot(x.T, delta2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step code Class\n",
    "class Neural_Network(object):\n",
    "    def __init__(self, input_layer_size, output_layer_size, hidden_layer_size):\n",
    "        self.input_layer_size = input_layer_size\n",
    "        self.output_layer_size = output_layer_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.W1 = np.random.randn(self.input_layer_size, self.hidden_layer_size)\n",
    "        self.W2 = np.random.randn(self.hidden_layer_size, self.output_layer_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        lr = 3\n",
    "        ### forward propagation\n",
    "        self.z2 = X.dot(self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = self.a2.dot(self.W2)\n",
    "        y_hat = self.sigmoid(self.z3)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def cost_function_prime(self, X, y):\n",
    "        y_hat = self.forward(X)\n",
    "        loss = self.MSE(y, y_hat)\n",
    "        \n",
    "        delta1 = np.multiply(-(y-y_hat), self.sigmoid_prime(self.z3))\n",
    "        w2_grad = np.dot(self.a2.T, delta1)\n",
    "        \n",
    "        delta2 = np.multiply(np.dot(delta1, self.W2.T), sigmoid_prime(self.z2))\n",
    "        w1_grad = np.dot(X.T, delta2)\n",
    "        return w1_grad, w2_grad\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def MSE(self, y, y_hat):\n",
    "        return np.power((y-y_hat),2)/2\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        return np.exp(-z)/np.power(1+np.exp(-z),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.3202136753487873\n",
      "epoch:1, loss:0.1876258590760205\n",
      "epoch:2, loss:0.040855604962388374\n",
      "epoch:3, loss:0.027857624362088804\n",
      "epoch:4, loss:0.0205007993497607\n",
      "epoch:5, loss:0.014637907403437954\n",
      "epoch:6, loss:0.009901939153286732\n",
      "epoch:7, loss:0.006473981337309021\n",
      "epoch:8, loss:0.004392158498030446\n",
      "epoch:9, loss:0.0033097142516499475\n",
      "epoch:10, loss:0.0027891533816184976\n",
      "epoch:11, loss:0.0025411239374557456\n",
      "epoch:12, loss:0.002419415320260904\n",
      "epoch:13, loss:0.0023564911005544203\n",
      "epoch:14, loss:0.0023215108396826304\n",
      "epoch:15, loss:0.0023001677884784753\n",
      "epoch:16, loss:0.0022856755259418942\n",
      "epoch:17, loss:0.002274742524715385\n",
      "epoch:18, loss:0.0022657377313805988\n",
      "epoch:19, loss:0.0022578345045580098\n",
      "epoch:20, loss:0.002250602477441116\n",
      "epoch:21, loss:0.00224380952323738\n",
      "epoch:22, loss:0.0022373243037240014\n",
      "epoch:23, loss:0.002231067752507039\n",
      "epoch:24, loss:0.002224988661540469\n",
      "epoch:25, loss:0.00221905125366761\n",
      "epoch:26, loss:0.002213228762232067\n",
      "epoch:27, loss:0.002207500043323558\n",
      "epoch:28, loss:0.002201847731745503\n",
      "epoch:29, loss:0.0021962571916053684\n",
      "epoch:30, loss:0.0021907158827779724\n",
      "epoch:31, loss:0.002185212950600088\n",
      "epoch:32, loss:0.002179738939978499\n",
      "epoch:33, loss:0.0021742855825496567\n",
      "epoch:34, loss:0.0021688456296370317\n",
      "epoch:35, loss:0.002163412716079832\n",
      "epoch:36, loss:0.002157981246377713\n",
      "epoch:37, loss:0.0021525462979460203\n",
      "epoch:38, loss:0.002147103538084479\n",
      "epoch:39, loss:0.002141649152279047\n",
      "epoch:40, loss:0.0021361797820588302\n",
      "epoch:41, loss:0.002130692471012256\n",
      "epoch:42, loss:0.002125184617826348\n",
      "epoch:43, loss:0.0021196539354016693\n",
      "epoch:44, loss:0.0021140984152411\n",
      "epoch:45, loss:0.002108516296426192\n",
      "epoch:46, loss:0.002102906038591079\n",
      "epoch:47, loss:0.0020972662983837296\n",
      "epoch:48, loss:0.002091595908972611\n",
      "epoch:49, loss:0.002085893862214724\n",
      "epoch:50, loss:0.002080159293151403\n",
      "epoch:51, loss:0.0020743914665407675\n",
      "epoch:52, loss:0.0020685897651734593\n",
      "epoch:53, loss:0.0020627536797499756\n",
      "epoch:54, loss:0.002056882800126242\n",
      "epoch:55, loss:0.002050976807757912\n",
      "epoch:56, loss:0.0020450354691950766\n",
      "epoch:57, loss:0.0020390586304971127\n",
      "epoch:58, loss:0.0020330462124534617\n",
      "epoch:59, loss:0.0020269982065095975\n",
      "epoch:60, loss:0.0020209146713098636\n",
      "epoch:61, loss:0.0020147957297787047\n",
      "epoch:62, loss:0.0020086415666714995\n",
      "epoch:63, loss:0.002002452426533706\n",
      "epoch:64, loss:0.0019962286120139504\n",
      "epoch:65, loss:0.00198997048248286\n",
      "epoch:66, loss:0.001983678452914587\n",
      "epoch:67, loss:0.001977352992992269\n",
      "epoch:68, loss:0.0019709946264031414\n",
      "epoch:69, loss:0.0019646039302918727\n",
      "epoch:70, loss:0.001958181534844222\n",
      "epoch:71, loss:0.001951728122975378\n",
      "epoch:72, loss:0.0019452444300997302\n",
      "epoch:73, loss:0.0019387312439610297\n",
      "epoch:74, loss:0.0019321894045032082\n",
      "epoch:75, loss:0.0019256198037640969\n",
      "epoch:76, loss:0.0019190233857754781\n",
      "epoch:77, loss:0.0019124011464539139\n",
      "epoch:78, loss:0.001905754133468256\n",
      "epoch:79, loss:0.001899083446070387\n",
      "epoch:80, loss:0.0018923902348766959\n",
      "epoch:81, loss:0.0018856757015888186\n",
      "epoch:82, loss:0.0018789410986424894\n",
      "epoch:83, loss:0.0018721877287746162\n",
      "epoch:84, loss:0.0018654169444988285\n",
      "epoch:85, loss:0.0018586301474808732\n",
      "epoch:86, loss:0.0018518287878056482\n",
      "epoch:87, loss:0.0018450143631283254\n",
      "epoch:88, loss:0.0018381884177029622\n",
      "epoch:89, loss:0.0018313525412822955\n",
      "epoch:90, loss:0.001824508367883507\n",
      "epoch:91, loss:0.0018176575744151917\n",
      "epoch:92, loss:0.001810801879161902\n",
      "epoch:93, loss:0.0018039430401229598\n",
      "epoch:94, loss:0.0017970828532035974\n",
      "epoch:95, loss:0.0017902231502569944\n",
      "epoch:96, loss:0.0017833657969768258\n",
      "epoch:97, loss:0.0017765126906409243\n",
      "epoch:98, loss:0.0017696657577075982\n",
      "epoch:99, loss:0.001762826951267055\n"
     ]
    }
   ],
   "source": [
    "input_layer_size = x.shape[1]\n",
    "output_layer_size = y.shape[1]\n",
    "hidden_layer_size = 4\n",
    "epoch = 100\n",
    "\n",
    "model = Neural_Network(input_layer_size, output_layer_size, hidden_layer_size)\n",
    "\n",
    "for i in range(epoch):\n",
    "    y_hat = model.forward(x)\n",
    "    w1_grad, w2_grad = model.cost_function_prime(x,y)\n",
    "    model.W1 -= w1_grad\n",
    "    model.W2 -= w2_grad\n",
    "    loss = model.MSE(y, y_hat)\n",
    "    \n",
    "    print(f'epoch:{i}, loss:{loss.sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연습 -  hidden layer를 1개 추가해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.23152475461096533\n",
      "epoch:1, loss:0.15766974787389665\n",
      "epoch:2, loss:0.09772924811112307\n",
      "epoch:3, loss:0.05899608003301593\n",
      "epoch:4, loss:0.03684072307008532\n",
      "epoch:5, loss:0.02443827518480203\n",
      "epoch:6, loss:0.017289649710039334\n",
      "epoch:7, loss:0.012983356732159414\n",
      "epoch:8, loss:0.01027345924320784\n",
      "epoch:9, loss:0.008501261179169644\n",
      "epoch:10, loss:0.0073039138833508365\n",
      "epoch:11, loss:0.006472601340809822\n",
      "epoch:12, loss:0.0058821251436344535\n",
      "epoch:13, loss:0.005454618787031148\n",
      "epoch:14, loss:0.0051400693349469865\n",
      "epoch:15, loss:0.004905438701790202\n",
      "epoch:16, loss:0.0047283610454081915\n",
      "epoch:17, loss:0.004593367987354231\n",
      "epoch:18, loss:0.004489559261297599\n",
      "epoch:19, loss:0.004409126296815854\n",
      "epoch:20, loss:0.004346393679490133\n",
      "epoch:21, loss:0.004297183272581397\n",
      "epoch:22, loss:0.004258384125315041\n",
      "epoch:23, loss:0.004227656437468627\n",
      "epoch:24, loss:0.004203224549495941\n",
      "epoch:25, loss:0.004183730098515181\n",
      "epoch:26, loss:0.004168126491165332\n",
      "epoch:27, loss:0.0041556021671228455\n",
      "epoch:28, loss:0.004145524195101228\n",
      "epoch:29, loss:0.004137396405531728\n",
      "epoch:30, loss:0.0041308280342519845\n",
      "epoch:31, loss:0.00412551004575815\n",
      "epoch:32, loss:0.0041211971212744385\n",
      "epoch:33, loss:0.004117693862503288\n",
      "epoch:34, loss:0.0041148441582666935\n",
      "epoch:35, loss:0.004112522942051594\n",
      "epoch:36, loss:0.004110629769463558\n",
      "epoch:37, loss:0.00410908378984355\n",
      "epoch:38, loss:0.004107819792218922\n",
      "epoch:39, loss:0.004106785083645541\n",
      "epoch:40, loss:0.004105937015726199\n",
      "epoch:41, loss:0.004105241018195668\n",
      "epoch:42, loss:0.0041046690308730325\n",
      "epoch:43, loss:0.004104198249811266\n",
      "epoch:44, loss:0.0041038101221510465\n",
      "epoch:45, loss:0.004103489538489497\n",
      "epoch:46, loss:0.004103224182587288\n",
      "epoch:47, loss:0.004103004006759456\n",
      "epoch:48, loss:0.004102820807920237\n",
      "epoch:49, loss:0.004102667884425711\n",
      "epoch:50, loss:0.004102539757914124\n",
      "epoch:51, loss:0.004102431947535691\n",
      "epoch:52, loss:0.004102340786485609\n",
      "epoch:53, loss:0.004102263272751272\n",
      "epoch:54, loss:0.004102196947573044\n",
      "epoch:55, loss:0.00410213979638357\n",
      "epoch:56, loss:0.0041020901680023925\n",
      "epoch:57, loss:0.004102046708672521\n",
      "epoch:58, loss:0.004102008308176608\n",
      "epoch:59, loss:0.004101974055793737\n",
      "epoch:60, loss:0.004101943204279408\n",
      "epoch:61, loss:0.004101915140392618\n",
      "epoch:62, loss:0.0041018893607685635\n",
      "epoch:63, loss:0.004101865452159523\n",
      "epoch:64, loss:0.004101843075246745\n",
      "epoch:65, loss:0.004101821951373491\n",
      "epoch:66, loss:0.004101801851668819\n",
      "epoch:67, loss:0.004101782588128577\n",
      "epoch:68, loss:0.004101764006299677\n",
      "epoch:69, loss:0.0041017459792780965\n",
      "epoch:70, loss:0.004101728402783471\n",
      "epoch:71, loss:0.00410171119111653\n",
      "epoch:72, loss:0.004101694273840612\n",
      "epoch:73, loss:0.004101677593056978\n",
      "epoch:74, loss:0.004101661101167492\n",
      "epoch:75, loss:0.004101644759037187\n",
      "epoch:76, loss:0.004101628534485062\n",
      "epoch:77, loss:0.004101612401044477\n",
      "epoch:78, loss:0.004101596336944618\n",
      "epoch:79, loss:0.004101580324273846\n",
      "epoch:80, loss:0.004101564348292213\n",
      "epoch:81, loss:0.004101548396866441\n",
      "epoch:82, loss:0.004101532460005837\n",
      "epoch:83, loss:0.0041015165294806845\n",
      "epoch:84, loss:0.004101500598508728\n",
      "epoch:85, loss:0.00410148466149751\n",
      "epoch:86, loss:0.004101468713832513\n",
      "epoch:87, loss:0.004101452751703113\n",
      "epoch:88, loss:0.004101436771959439\n",
      "epoch:89, loss:0.00410142077199477\n",
      "epoch:90, loss:0.004101404749648827\n",
      "epoch:91, loss:0.004101388703128289\n",
      "epoch:92, loss:0.004101372630941435\n",
      "epoch:93, loss:0.004101356531844432\n",
      "epoch:94, loss:0.004101340404797181\n",
      "epoch:95, loss:0.004101324248926985\n",
      "epoch:96, loss:0.0041013080634987615\n",
      "epoch:97, loss:0.004101291847890401\n",
      "epoch:98, loss:0.004101275601572689\n",
      "epoch:99, loss:0.004101259324092657\n"
     ]
    }
   ],
   "source": [
    "### step code\n",
    "x = np.array([[3, 5],[5, 1],[10, 2]])\n",
    "y = np.array([[0.75], [0.82], [0.93]])\n",
    "w1 = np.random.randn(2,4)\n",
    "w2 = np.random.randn(4,4)\n",
    "w3 = np.random.randn(4,1)\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    z1 = x.dot(w1)\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1.dot(w2) \n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = a2.dot(w3)\n",
    "    y_hat = sigmoid(z3)\n",
    "    loss = MSE(y,y_hat)/2\n",
    "    \n",
    "    delta1 = (y_hat-y)*sigmoid_prime(z3)\n",
    "    w3_gred = a2.T.dot(delta1)\n",
    "\n",
    "    delta2 = np.dot(delta1, w3.T) * sigmoid_prime(z2)\n",
    "    w2_gred = np.dot(a1.T, delta2)\n",
    "\n",
    "    delta3 = np.dot(delta2, w2.T) * sigmoid_prime(z1)\n",
    "    w1_gred = np.dot(x.T, delta3)\n",
    "\n",
    "    w3 -= w3_gred\n",
    "    w2 -= w2_gred\n",
    "    w1 -= w1_gred\n",
    "    \n",
    "    print(f'epoch:{i}, loss:{loss.sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연습2 - activation function relu로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leru(z):\n",
    "    z[z<0] = 0\n",
    "    return z1\n",
    "\n",
    "def leru_prime(z):\n",
    "    z[z<0] = 0\n",
    "    z[z>0] = 1\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step code\n",
    "x = np.array([[3, 5],[5, 1],[10, 2]])\n",
    "y = np.array([[0.75], [0.82], [0.93]])\n",
    "w1 = np.random.randn(2,4)\n",
    "w2 = np.random.randn(4,4)\n",
    "w3 = np.random.randn(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leru_prime(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
